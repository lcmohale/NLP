{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Classify Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook what im aiming to Learn is how we can Classify Text, this basically means how we can Say that a piece of Text falls to a certain Category of writting...\n",
    "\n",
    "This topic of classifiying Text falls under the Machine Learning Topic (Classification) and the type of classification which I am doing here is Supervided text Classification, which means that my model before it can predict it's results from new Input data should first have an idea of what a correct laballing (categorization) looks like(training)\n",
    "\n",
    "- \"A classifier is called supervised if it is built based on training corpora containing the correct label for each input.\"\n",
    "\n",
    "- The first step in creating a classifier is deciding what features of the input are relevant, and how to encode those features   this is whats called as Feature Extraction.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Feature_extraction:\n",
    "- When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters, or the repetitiveness of images presented as pixels), then it can be transformed into a reduced set of features (also named a feature vector). Determining a subset of the initial features is called feature selection.The selected features are expected to contain the relevant information from the input data, so that the desired task can be performed by using this reduced representation instead of the complete initial data\n",
    "\n",
    "As a test case we are going to create a feature extractor to extract features from names data, the feature we are going to extract is going to be the last letter of a name, since there has been previous observation that Female and Male Names, tend to be different from each other, based on the last letter of the name.\n",
    "\n",
    "- we saw that male and female names have some distinctive characteristics. Names ending in a, e and i are likely to be female, while names ending in k, o, r, s and t are likely to be male.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'k'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test Feature Extractor \n",
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}\n",
    "\n",
    "gender_features('shrek')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of a Feature extractor is a Dictionary which is usually called 'feature set'\n",
    "\n",
    "https://en.wikipedia.org/wiki/Feature_(machine_learning)\n",
    "- In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition.\n",
    "\n",
    "Now a feature set is basically a set of Feature's.. in this case feature's resulting from feature extraction..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have created our Feature Extraction Function, we now have to have some data to test it with, we are going to use names data that comes with NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import names\n",
    "\n",
    "#This returns a list of tuples each containing a name and it's label\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "import random\n",
    "random.shuffle(labeled_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use our feature extractor to process the names data, and divide the resulting list of feature sets into a training set and a test set. The training set is used to train a new \"naive Bayes\" classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "\n",
    "#Note the NaiveBayesClassifier.train is given a list of labeled data\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "classifier.classify(gender_features('apple'))\n",
    "\n",
    "# We can systematically evaluate the classifier on a much larger quantity of unseen data:\n",
    "#print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "#Finally, we can examine the classifier to determine which features it found most effective for distinguishing the names' genders:\n",
    "#classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This listing shows that the names in the training set that end in \"a\" are female 33 times more often than they are male, but names that end in \"k\" are male 32 times more often than they are female. These ratios are known as likelihood ratios, and can be useful for comparing different feature-outcome relationships\n",
    "\n",
    "- When working with large corpora, constructing a single list that contains the features of every instance can use up a large amount of memory. In these cases, use the function nltk.classify.apply_features, which returns an object that acts like a list but does not store all the feature sets in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
