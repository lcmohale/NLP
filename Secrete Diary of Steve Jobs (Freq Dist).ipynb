{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on The Secret Diary of Steve Jobs\n",
    "\n",
    "In this article what im trying to Archive is to try and Find Steve Jobs's view of Jesus Christ.(based on this text).\n",
    "Im not sure at the time of this writing how I plan on going on with this..but I believe ultimately I will arrive at a certain conclusion, which might be different from my initial plan or what I thought I would archive as stated above!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Story..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I can't really recall what i was looking for when I bumped to the site www.fakesteve.net, I certainlly didn't put \"Steve jobs\" in the search bar, No!.. my interests on the Man dont really go that far, as to do a search which will eventually lead me to find fakesteve.net a site with the Main heading that states \"The Secret Diary of Steve Jobs\".\n",
    "\n",
    "\"The Secret Diary of Steve Jobs\"...the title certainly had an influence on my decision to go through the site reading and simply not scroll through and leave, but not only that I guess, cause each time i arrived at the end of one article i just wanted to go to another one, so i did enjoy reading \"The secret diary of steve Jobs\".\n",
    "\n",
    "\n",
    "But one article that really caught my attention was the one with this attributes: \n",
    "- Posted by Steve at 6:55 am\n",
    "- Monday, December 6, 2010\n",
    "- Hate-spewing “Christians” need to listen up\n",
    "\n",
    "I read the post till the end and didn't ever read it again, so my view of this article is kinda vague, which is one thing that makes me wanna use some NLTK functionalities to form a coherent view of the article.\n",
    "\n",
    "\n",
    "So my plan is to Scrape the Site, obtain the article and do some NLTK/NLP stuff on it...oh by the way im certainly a novice in this amazing field of Natural Language Processing, web scraping and NLTK even blogging, but i do have the theory of the subjects, so this forms part of obtaining the Practicals to accompany the theory and gain some experience.\n",
    ":) read till the end please.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I have Limited Internet access..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have limited Internet access!; What i mean is that I can only access the internet at certain places,\n",
    "but not from home, which is where I'm actually writing this.\n",
    "\n",
    "So im going to scrape the Site a bit differently, since I had saved the page of interest to my local machine..\n",
    "But hopefully i will also do another version of this writing with the page of interest where it's actually located\n",
    " www.fakesteve.net., I also hope im not stepping on anyone's toes for doing this also.\n",
    "\n",
    "Okay on to the Code...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -Scraping the Site/ Obtaining the Data:\n",
    "\n",
    "#Get the File (savedPage)\n",
    "fileUrl = \"C:\\\\Users\\\\Admin\\\\Downloads\\\\SavedPages\\\\The Secret Diary of Steve Jobs.html\"\n",
    "savedPage = open(fileUrl,'rb')\n",
    "\n",
    "#Use BeautifulSoup to Scrape the Page\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "pageAsSoup = BeautifulSoup(savedPage, \"html5lib\")\n",
    "pageAsText = pageAsSoup.get_text()\n",
    "\n",
    "#..wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "...From printing the varible 'pageAsText' I see the stucture of the page,...in a page there are multiple blog posts, but im much more interested in a particular post, so i'll have to dig into the page and take out that chunk that i want to do some checks on!\n",
    "\n",
    "The Nice :) thing is that the entire page is now just a bunch of text (string), so you can just do some indexing and get your chunk!...\n",
    "\n",
    "Now, in order to do this I had to re-open the page (saved on disk) in a web browser.. and actually do some \"Eye Analysis\" on the data..\n",
    "\n",
    "I had to find the Text that starts my Post of Interest, and the Text which ends the post, in order to find the location of the post of interest within the whole page.CHOP! CHOP!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the required Post\n",
    "start_of_required_post =  pageAsText.index('Hate-spewing')\n",
    "end_of_required_post =  pageAsText.index('happier')\n",
    "required_post_ending_word =  'happier'\n",
    "\n",
    "requiredPost = pageAsText[start_of_required_post : end_of_required_post ]\n",
    "requiredPost = requiredPost + required_post_ending_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "... I have decided to find the most frequent word in the Post, Using NLTK,... now that i have got my text :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find The Most Frequent Words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the goal now is to find the most frequent words in the blog  post..there are certain things to understand..,\n",
    "\n",
    "- What we have now is a group of text / string (requiredPost), but the goal is to find the most frequent \"words\" in the post...\n",
    "- this means that we have to break down the string to it's tokens (words)...\n",
    "- this task can be simply archieved using NLTK..\n",
    "\n",
    "This process of breaking down a group of text to it's tokens is called Tokenization \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#- Word tokenization \n",
    "#Create word tokens from a group of text (in this case the blog post)\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "tokens = tokenizer.tokenize(requiredPost)\n",
    "\n",
    "# Create a list to store the words, this words are the result from tokenizing the text (blog post) using NLTK from code above\n",
    "words = []\n",
    "\n",
    "#...wait "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK tokenizer used above to tokenize the text is the 'RegexpTokenizer' and it uses Regular Expressions for its tokenizing process\n",
    "\n",
    "The RegexpTokenizer is provided with the pattern to find words ('\\w+'), ignoring the case...\n",
    "this means that the result of the RegexpTokenizer will be words both lower and uppercase..\n",
    "\n",
    "this result could lead to a problem since the words The, THERE, and there would be considered different from each other so they wont all add to the count of the word 'the' when counting the word frequency...\n",
    "\n",
    "In order to solve this we have to standardize all the words to lower case...\n",
    "so, the words 'The', 'THERE' and 'the' would be considered as one word 'the', when we count the number of occurances the word 'the' appears in the article...\n",
    "\n",
    "..in code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8444b2d973e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#- Make all the Word tokens to be lower case and store them in a list 'words'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#...continued\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "#- Make all the Word tokens to be lower case and store them in a list 'words'\n",
    "for word in tokens:\n",
    "    words.append(word.lower())\n",
    "\n",
    "#...continued\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When counting word frequency we have to take into consideration, what we consider as words, would we consider 'I','Me','It' to be words?...\n",
    "\n",
    "Yes this are words but, this words occur so often in any piece of text and they don't actually give us the gist of the text, well maybe they do when we want to find the context of the text. but in the case of finding the most frequent words.. from a text this words (Stop words) will actually cloud our results since they usually appear with the most frequency..in most texts..\n",
    "\n",
    "Here's an extract from the site http://www.rxnlp.com/why-use-stop-words-for-text-mining/ which tells why stop words are usually removed from text before doing some processing...\n",
    "\n",
    "\"When working with text mining applications, we often hear of the term “stop words” or “stop word list” or even “stop list”. Stop words are just a set of commonly used words in any language. Stop words are commonly eliminated from many text processing applications because these words can be distracting, non-informative (or non-discriminative) and are additional memory overhead.\"\n",
    "\n",
    "In order to Remove this words from our text.. we have to have them or get them somewhere.\n",
    "Stop Words lists can be downloaded from various sources on the Internet but for our current case we are going to acquire them from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# - Remove Stopwords from the Text..\n",
    "# - We going to use NLTK stop words\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Get English english stopwords from NLTK \n",
    "stopWords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Initialize new list to store the words without stop words.\n",
    "justWords_withoutStopWords = []\n",
    "\n",
    "# Add to justWords_withoutStopWords all words that are in words but not in stopWords\n",
    "# Basically store the entire text you working with but without the stop words\n",
    "for word in words:\n",
    "    if word not in stopWords:\n",
    "        justWords_withoutStopWords.append(word)\n",
    "    \n",
    "#...continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since now I have the Text (blogPost) tokenized to words, and have removed stop words we can now actually start finding the most Frequent words / the word Frequency Distribution..of the Text..\n",
    "NLTK again provides the functionality to archive this task using it's FreqDist function...\n",
    "\n",
    "after we have computed the Frequent words I am going to plot the results.. and for this task I'll use matplotlib and seaborn\n",
    "Python libraries...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Import data visualisation libraries for plotting the results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Figures inline and set visualization style\n",
    "# This two following lines of code are recommended since im using Jupyter Notebook,\n",
    "# They basically enhance the plotting functionalities of Jupyter\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "# Compute the freq using NLTK FreqDist and plot the results...\n",
    "freqdist1 = nltk.FreqDist(justWords_withoutStopWords)\n",
    "freqdist1.plot(25)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
